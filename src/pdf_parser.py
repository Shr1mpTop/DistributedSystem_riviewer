"""
PDFè§£ææ¨¡å— - ä½¿ç”¨Google AIæå–è€ƒè¯•é¢˜ç›®
ä½œè€…: åˆ†å¸ƒå¼ç³»ç»Ÿè€ƒè¯•æŒ‡å—é¡¹ç›®ç»„
åŠŸèƒ½: è§£æPDFæ–‡ä»¶ä¸­çš„è€ƒè¯•é¢˜ç›®ï¼Œè¾“å‡ºæ ‡å‡†åŒ–JSONæ ¼å¼
"""

import os
import json
import asyncio
import logging
import time
from typing import List, Dict, Any
from pathlib import Path
from google import genai
from google.genai import types
from dotenv import load_dotenv
from tqdm import tqdm

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class PDFParser:
    def __init__(self):
        """åˆå§‹åŒ–PDFè§£æå™¨"""
        self.api_key = os.getenv('GOOGLE_API_KEY')
        if not self.api_key:
            raise ValueError("è¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®GOOGLE_API_KEY")
        
        # é…ç½®Google AIå®¢æˆ·ç«¯
        self.client = genai.Client(api_key=self.api_key)
        
        # é…ç½®æ—¥å¿—
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        # åŠ è½½è¯¾ç¨‹å¤§çº²ç”¨äºçŸ¥è¯†ç‚¹æ˜ å°„
        self.curriculum = self._load_curriculum()
        
        # APIé€Ÿç‡é™åˆ¶æ§åˆ¶
        self.last_api_call = 0
        self.min_interval = 30  # å…è´¹ç‰ˆé™åˆ¶ï¼šæ¯åˆ†é’Ÿ2æ¬¡è¯·æ±‚ï¼Œå®‰å…¨èµ·è§30ç§’ä¸€æ¬¡
    
    def _load_curriculum(self) -> Dict[str, Any]:
        """åŠ è½½è¯¾ç¨‹å¤§çº²JSONæ•°æ®"""
        try:
            curriculum_path = Path('data/curriculum.json')
            with open(curriculum_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            self.logger.warning(f"æ— æ³•åŠ è½½è¯¾ç¨‹å¤§çº²: {e}")
            return {}
    
    def wait_for_rate_limit(self):
        """æ§åˆ¶APIè°ƒç”¨é€Ÿç‡ï¼Œé¿å…è¶…å‡ºé™åˆ¶"""
        current_time = time.time()
        elapsed = current_time - self.last_api_call
        if elapsed < self.min_interval:
            wait_time = self.min_interval - elapsed
            self.logger.info(f"ç­‰å¾… {wait_time:.1f} ç§’ä»¥é¿å…é€Ÿç‡é™åˆ¶...")
            time.sleep(wait_time)
        self.last_api_call = time.time()
    
    def create_extraction_prompt(self) -> str:
        """åˆ›å»ºç”¨äºé¢˜ç›®æå–çš„è¶…çº§ä¸¥æ ¼æç¤ºè¯ - ä¸ºäº†ç™¾ä¸‡å¹´è–ªï¼"""
        curriculum_str = json.dumps(self.curriculum, ensure_ascii=False, indent=2)
        
        prompt = f"""
## ğŸ¯ CRITICAL INSTRUCTION - MUST FOLLOW EXACTLY ğŸ¯

ä½ æ˜¯ä¸€ä¸ªé¡¶çº§çš„è€ƒè¯•é¢˜ç›®æå–ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä»è¿™ä¸ªPDFæ–‡æ¡£ä¸­æå–è€ƒè¯•é¢˜ç›®ï¼Œå¹¶ä¸”å¿…é¡»ä¸¥æ ¼æŒ‰ç…§æŒ‡å®šçš„JSONæ ¼å¼è¾“å‡ºã€‚

**âš ï¸ é‡è¦ï¼šä½ çš„å›å¤å¿…é¡»æ˜¯çº¯JSONæ ¼å¼ï¼Œä¸èƒ½åŒ…å«ä»»ä½•å…¶ä»–æ–‡å­—è¯´æ˜ï¼**

### ğŸ“š è¯¾ç¨‹å¤§çº²å‚è€ƒï¼ˆç”¨äºåŒ¹é…çŸ¥è¯†ç‚¹ï¼‰:
{curriculum_str}

### ğŸ”’ å¼ºåˆ¶è¾“å‡ºæ ¼å¼ - å¿…é¡»ä¸¥æ ¼éµå®ˆï¼š
{{
  "questions": [
    {{
      "id": "Q001",
      "title": "é¢˜ç›®çš„å®Œæ•´æè¿°ï¼ŒåŒ…æ‹¬æ‰€æœ‰é€‰é¡¹ï¼ˆå¦‚æœæ˜¯é€‰æ‹©é¢˜ï¼‰",
      "type": "é€‰æ‹©é¢˜|å¡«ç©ºé¢˜|ç®€ç­”é¢˜|è®ºè¿°é¢˜|è®¡ç®—é¢˜|åˆ¤æ–­é¢˜|ç¼–ç¨‹é¢˜",
      "answer": "æ ‡å‡†ç­”æ¡ˆæˆ–æœªæä¾›",
      "refer": "å¯¹åº”çš„è¯¾ç¨‹ç« èŠ‚åç§°"
    }}
  ]
}}

### ğŸ“‹ ä¸¥æ ¼æ‰§è¡Œè§„åˆ™ï¼š
1. **å¿…é¡»è¾“å‡ºæœ‰æ•ˆJSON** - ä»»ä½•éJSONå†…å®¹éƒ½ä¸è¢«æ¥å—
2. **idæ ¼å¼**: Q001, Q002, Q003... ä¾æ¬¡é€’å¢
3. **title**: åŒ…å«é¢˜ç›®å®Œæ•´å†…å®¹ï¼Œé€‰æ‹©é¢˜è¦åŒ…å«æ‰€æœ‰é€‰é¡¹Aã€Bã€Cã€D
4. **type**: åªèƒ½æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼šé€‰æ‹©é¢˜ã€å¡«ç©ºé¢˜ã€ç®€ç­”é¢˜ã€è®ºè¿°é¢˜ã€è®¡ç®—é¢˜ã€åˆ¤æ–­é¢˜ã€ç¼–ç¨‹é¢˜
5. **answer**: å¦‚æœPDFä¸­æœ‰ç­”æ¡ˆå°±å†™ç­”æ¡ˆï¼Œæ²¡æœ‰å°±å†™"æœªæä¾›"
6. **refer**: æ ¹æ®é¢˜ç›®å†…å®¹åŒ¹é…åˆ°è¯¾ç¨‹å¤§çº²ä¸­çš„ç« èŠ‚ï¼Œæ ¼å¼å¦‚"ç¬¬1ç«  åˆ†å¸ƒå¼ç³»ç»Ÿç‰¹å¾ä¸ç³»ç»Ÿæ¨¡å‹"

### ğŸ¯ é¢˜ç›®è¯†åˆ«æ¨¡å¼ï¼š
- å¯»æ‰¾åºå·ï¼š1., 2., (1), (2), Q1, Question 1 ç­‰
- è¯†åˆ«é—®å·ã€é€‰æ‹©é¡¹ã€å¡«ç©ºçº¿
- å¯»æ‰¾"ç­”æ¡ˆ"ã€"è§£ç­”"ã€"Answer"ç­‰å…³é”®è¯

### âš¡ è¾“å‡ºè¦æ±‚ï¼š
- ç›´æ¥è¾“å‡ºJSONï¼Œä¸è¦åŒ…è£…åœ¨```json```ä»£ç å—ä¸­
- ä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæ–‡å­—
- ç¡®ä¿JSONæ ¼å¼å®Œå…¨æ­£ç¡®ï¼Œå¯è¢«ç¨‹åºè§£æ
- å¦‚æœæ²¡æ‰¾åˆ°é¢˜ç›®ï¼Œè¾“å‡ºï¼š{{"questions": []}}

è¯·ä»”ç»†åˆ†æè¿™ä¸ªPDFæ–‡æ¡£å¹¶æå–æ‰€æœ‰è€ƒè¯•é¢˜ç›®ï¼š"""
        return prompt
    
    async def analyze_pdf_with_ai(self, pdf_path: str, pdf_name: str, max_retries: int = 3) -> Dict[str, Any]:
        """ä½¿ç”¨Google AIç›´æ¥åˆ†æPDFæ–‡ä»¶å¹¶æå–é¢˜ç›®"""
        for attempt in range(max_retries):
            try:
                # æ§åˆ¶APIè°ƒç”¨é€Ÿç‡
                self.wait_for_rate_limit()
                
                # åˆ›å»ºæç¤ºè¯
                prompt = self.create_extraction_prompt()
                
                self.logger.info(f"æ­£åœ¨åˆ†æ {pdf_name}... (å°è¯• {attempt + 1}/{max_retries})")
                
                # è¯»å–PDFæ–‡ä»¶ä¸ºå­—èŠ‚æ•°æ®
                pdf_path_obj = Path(pdf_path)
                pdf_bytes = pdf_path_obj.read_bytes()
                
                # ä½¿ç”¨æ–°çš„Google AI APIç›´æ¥å¤„ç†PDF
                response = await asyncio.to_thread(
                    self.client.models.generate_content,
                    model="gemini-2.5-flash",  # ä½¿ç”¨flashç‰ˆæœ¬ï¼Œæ›´å¿«æ›´ä¾¿å®œ
                    contents=[
                        types.Part.from_bytes(
                            data=pdf_bytes,
                            mime_type='application/pdf',
                        ),
                        prompt
                    ]
                )
                
                # ğŸ“‹ è¶…çº§æ™ºèƒ½JSONè§£æ - ä¸ºäº†ç™¾ä¸‡å¹´è–ªï¼
                response_text = response.text.strip()
                self.logger.info(f"AIå“åº”é•¿åº¦: {len(response_text)} å­—ç¬¦")
                
                # ğŸ” å¤šç§æ–¹å¼å°è¯•æå–JSON
                json_candidates = []
                
                # æ–¹å¼1: å¯»æ‰¾```jsonä»£ç å—
                if '```json' in response_text:
                    json_start = response_text.find('```json') + 7
                    json_end = response_text.find('```', json_start)
                    if json_end > json_start:
                        json_candidates.append(response_text[json_start:json_end].strip())
                
                # æ–¹å¼2: å¯»æ‰¾ç¬¬ä¸€ä¸ª{åˆ°æœ€åä¸€ä¸ª}
                if '{' in response_text and '}' in response_text:
                    json_start = response_text.find('{')
                    json_end = response_text.rfind('}') + 1
                    json_candidates.append(response_text[json_start:json_end])
                
                # æ–¹å¼3: å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•æ•´ä¸ªå“åº”
                json_candidates.append(response_text)
                
                # ğŸ¯ å°è¯•è§£ææ¯ä¸ªå€™é€‰JSON
                result = None
                for i, candidate in enumerate(json_candidates):
                    try:
                        result = json.loads(candidate)
                        self.logger.info(f"JSONè§£ææˆåŠŸ (æ–¹å¼ {i+1})")
                        break
                    except json.JSONDecodeError:
                        continue
                
                if result is None:
                    raise json.JSONDecodeError("æ‰€æœ‰JSONè§£ææ–¹å¼éƒ½å¤±è´¥", response_text, 0)
                
                # âœ… éªŒè¯å’Œä¿®å¤ç»“æœæ ¼å¼
                if not isinstance(result, dict):
                    result = {'questions': []}
                elif 'questions' not in result:
                    result = {'questions': []}
                elif not isinstance(result['questions'], list):
                    result = {'questions': []}
                
                # ğŸ”§ éªŒè¯æ¯ä¸ªé¢˜ç›®æ ¼å¼
                valid_questions = []
                for i, question in enumerate(result['questions']):
                    if isinstance(question, dict):
                        # ç¡®ä¿æ‰€æœ‰å¿…éœ€å­—æ®µå­˜åœ¨
                        fixed_question = {
                            'id': question.get('id', f'Q{i+1:03d}'),
                            'title': str(question.get('title', '')).strip(),
                            'type': str(question.get('type', 'æœªçŸ¥é¢˜å‹')).strip(),
                            'answer': str(question.get('answer', 'æœªæä¾›')).strip(),
                            'refer': str(question.get('refer', 'æœªåˆ†ç±»')).strip()
                        }
                        
                        # åªä¿ç•™æœ‰æ•ˆé¢˜ç›®ï¼ˆè‡³å°‘æœ‰æ ‡é¢˜ï¼‰
                        if fixed_question['title']:
                            valid_questions.append(fixed_question)
                
                result['questions'] = valid_questions
                
                self.logger.info(f"{pdf_name} è§£æå®Œæˆï¼Œæå–åˆ° {len(result['questions'])} é“é¢˜ç›®")
                return result
                
            except json.JSONDecodeError as e:
                self.logger.error(f"{pdf_name} JSONè§£æå¤±è´¥ (å°è¯• {attempt + 1}): {e}")
                if 'response_text' in locals():
                    self.logger.error(f"AIå“åº”å†…å®¹: {response_text[:500]}...")
                if attempt == max_retries - 1:
                    return {'questions': []}
            except Exception as e:
                error_msg = str(e)
                if "429" in error_msg or "quota" in error_msg.lower():
                    self.logger.warning(f"{pdf_name} é‡åˆ°é€Ÿç‡é™åˆ¶ (å°è¯• {attempt + 1}): {e}")
                    if attempt < max_retries - 1:
                        # ä»é”™è¯¯æ¶ˆæ¯ä¸­æå–é‡è¯•å»¶è¿Ÿ
                        import re
                        retry_match = re.search(r'retry in (\d+(?:\.\d+)?)s', error_msg)
                        if retry_match:
                            retry_delay = float(retry_match.group(1)) + 1  # æ·»åŠ é¢å¤–1ç§’ç¼“å†²
                            self.logger.info(f"ç­‰å¾… {retry_delay:.1f} ç§’åé‡è¯•...")
                            await asyncio.sleep(retry_delay)
                        else:
                            await asyncio.sleep(60)  # é»˜è®¤ç­‰å¾…60ç§’
                        continue
                else:
                    self.logger.error(f"{pdf_name} AIåˆ†æå¤±è´¥ (å°è¯• {attempt + 1}): {e}")
                    if attempt == max_retries - 1:
                        return {'questions': []}
        
        return {'questions': []}
    
    async def parse_single_pdf(self, pdf_path: str) -> Dict[str, Any]:
        """è§£æå•ä¸ªPDFæ–‡ä»¶"""
        pdf_name = Path(pdf_path).name
        self.logger.info(f"å¼€å§‹å¤„ç†: {pdf_name}")
        
        # æ£€æŸ¥PDFæ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”å¤§å°åˆé€‚
        pdf_file = Path(pdf_path)
        if not pdf_file.exists():
            self.logger.error(f"PDFæ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
            return {'questions': []}
        
        file_size_mb = pdf_file.stat().st_size / (1024 * 1024)
        if file_size_mb > 20:
            self.logger.warning(f"{pdf_name} æ–‡ä»¶è¿‡å¤§ ({file_size_mb:.1f}MB)ï¼Œå¯èƒ½ä¼šå¤„ç†å¤±è´¥")
        
        # ä½¿ç”¨AIç›´æ¥åˆ†æPDF
        result = await self.analyze_pdf_with_ai(pdf_path, pdf_name)
        
        # ä¸ºæ¯ä¸ªé¢˜ç›®æ·»åŠ æºæ–‡ä»¶ä¿¡æ¯
        for question in result.get('questions', []):
            question['source'] = pdf_name
        
        return result
    
    async def parse_all_pdfs(self, pdf_directory: str = ".") -> List[Dict[str, Any]]:
        """ä¸²è¡Œè§£ææ‰€æœ‰PDFæ–‡ä»¶ï¼ˆé¿å…APIé€Ÿç‡é™åˆ¶ï¼‰"""
        pdf_files = list(Path(pdf_directory).glob("*.pdf"))
        
        if not pdf_files:
            self.logger.warning("æœªæ‰¾åˆ°PDFæ–‡ä»¶")
            return []
        
        self.logger.info(f"å‘ç° {len(pdf_files)} ä¸ªPDFæ–‡ä»¶")
        self.logger.info("ä½¿ç”¨ä¸²è¡Œå¤„ç†ä»¥é¿å…APIé€Ÿç‡é™åˆ¶...")
        
        results = []
        
        # ä½¿ç”¨è¿›åº¦æ¡æ˜¾ç¤ºå¤„ç†è¿›åº¦
        with tqdm(total=len(pdf_files), desc="è§£æPDFæ–‡ä»¶") as pbar:
            for i, pdf_file in enumerate(pdf_files):
                self.logger.info(f"å¤„ç†ç¬¬ {i+1}/{len(pdf_files)} ä¸ªæ–‡ä»¶: {pdf_file.name}")
                
                # åœ¨æ¯ä¸ªæ–‡ä»¶ä¹‹é—´æ·»åŠ çŸ­æš‚å»¶è¿Ÿ
                if i > 0:
                    await asyncio.sleep(2)  # 2ç§’å»¶è¿Ÿä»¥é¿å…é€Ÿç‡é™åˆ¶
                
                result = await self.parse_single_pdf(str(pdf_file))
                results.append(result)
                pbar.update(1)
        
        return results
    
    def save_results(self, results: List[Dict[str, Any]], output_path: str = "output/parsed_questions.json"):
        """ä¿å­˜è§£æç»“æœåˆ°JSONæ–‡ä»¶"""
        # åˆå¹¶æ‰€æœ‰ç»“æœ
        all_questions = []
        for result in results:
            all_questions.extend(result.get('questions', []))
        
        # ä¸ºé¢˜ç›®é‡æ–°ç¼–å·
        for i, question in enumerate(all_questions, 1):
            question['id'] = f"Q{i:03d}"
        
        # ä¿å­˜ç»“æœ
        output_data = {
            'total_questions': len(all_questions),
            'questions': all_questions,
            'metadata': {
                'extracted_at': str(Path().resolve()),
                'pdf_count': len(results)
            }
        }
        
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"è§£æç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        self.logger.info(f"æ€»å…±æå–åˆ° {len(all_questions)} é“é¢˜ç›®")
        
        return output_data

async def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºPDFè§£æåŠŸèƒ½"""
    parser = PDFParser()
    
    # è§£ææ‰€æœ‰PDFæ–‡ä»¶
    results = await parser.parse_all_pdfs()
    
    # ä¿å­˜ç»“æœ
    parser.save_results(results)

if __name__ == "__main__":
    asyncio.run(main())