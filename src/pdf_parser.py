"""
PDFè§£ææ¨¡å— - ä½¿ç”¨Google AIæå–è€ƒè¯•é¢˜ç›®
ä½œè€…: åˆ†å¸ƒå¼ç³»ç»Ÿè€ƒè¯•æŒ‡å—é¡¹ç›®ç»„
åŠŸèƒ½: è§£æPDFæ–‡ä»¶ä¸­çš„è€ƒè¯•é¢˜ç›®ï¼Œè¾“å‡ºæ ‡å‡†åŒ–JSONæ ¼å¼
"""

import os
import json
import asyncio
import logging
import time
from typing import List, Dict, Any
from pathlib import Path
from google import genai
from google.genai import types
from dotenv import load_dotenv
from tqdm import tqdm

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class PDFParser:
    def __init__(self):
        """åˆå§‹åŒ–PDFè§£æå™¨"""
        self.api_key = os.getenv('GOOGLE_API_KEY')
        if not self.api_key:
            raise ValueError("è¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®GOOGLE_API_KEY")
        
        # é…ç½®Google AIå®¢æˆ·ç«¯
        self.client = genai.Client(api_key=self.api_key)
        
        # é…ç½®æ—¥å¿—
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        # åŠ è½½è¯¾ç¨‹å¤§çº²ç”¨äºçŸ¥è¯†ç‚¹æ˜ å°„
        self.curriculum = self._load_curriculum()
        
        # APIé€Ÿç‡é™åˆ¶æ§åˆ¶
        self.last_api_call = 0
        self.min_interval = 30  # å…è´¹ç‰ˆé™åˆ¶ï¼šæ¯åˆ†é’Ÿ2æ¬¡è¯·æ±‚ï¼Œå®‰å…¨èµ·è§30ç§’ä¸€æ¬¡
    
    def _load_curriculum(self) -> Dict[str, Any]:
        """åŠ è½½è¯¾ç¨‹å¤§çº²JSONæ•°æ®"""
        try:
            curriculum_path = Path('data/curriculum.json')
            with open(curriculum_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            self.logger.warning(f"æ— æ³•åŠ è½½è¯¾ç¨‹å¤§çº²: {e}")
            return {}
    
    def wait_for_rate_limit(self):
        """æ§åˆ¶APIè°ƒç”¨é€Ÿç‡ï¼Œé¿å…è¶…å‡ºé™åˆ¶"""
        current_time = time.time()
        elapsed = current_time - self.last_api_call
        if elapsed < self.min_interval:
            wait_time = self.min_interval - elapsed
            self.logger.info(f"ç­‰å¾… {wait_time:.1f} ç§’ä»¥é¿å…é€Ÿç‡é™åˆ¶...")
            time.sleep(wait_time)
        self.last_api_call = time.time()
    
    def create_extraction_prompt(self) -> str:
        """Create extraction prompt in English for better Gemini performance"""
        curriculum_str = json.dumps(self.curriculum, ensure_ascii=False, indent=2)

        prompt = f"""
## ğŸ¯ CRITICAL INSTRUCTION - MUST FOLLOW EXACTLY ğŸ¯

You are a top-tier exam question extraction expert. Your task is to extract exam questions from this PDF document and output them in the specified JSON format strictly.

**âš ï¸ IMPORTANT: Your response must be pure JSON format without any other text explanation!**

### ğŸ“š Course Curriculum Reference (for knowledge point matching):
{curriculum_str}

### ğŸ”’ MANDATORY OUTPUT FORMAT - MUST STRICTLY FOLLOW:
{{
  "questions": [
    {{
      "id": "Q001",
      "title": "Complete question description including all sub-questions (a), (b), (c) if they belong to the same main question",
      "type": "Multiple Choice|Fill in Blank|Short Answer|Essay|Calculation|True/False|Programming",
      "refer": "Corresponding course chapter name",
      "knowledge_points": ["specific knowledge point 1", "specific knowledge point 2", "specific knowledge point 3"]
    }}
  ]
}}

### ğŸ“‹ STRICT EXECUTION RULES:
1. **MUST output valid JSON** - No non-JSON content is accepted
2. **id format**: Q001, Q002, Q003... incrementally
3. **title**: Complete question description for each individual sub-question
4. **type**: Must be one of: Multiple Choice, Fill in Blank, Short Answer, Essay, Calculation, True/False, Programming
5. **refer**: Match to curriculum chapters based on question content, format like "Chapter 1 Characterization of Distributed Systems & System Models"
6. **knowledge_points**: Must be array format, containing specific knowledge points related to this question

### ğŸ¯ QUESTION IDENTIFICATION PATTERNS - CRITICAL FOR SUB-QUESTION EXTRACTION:
- **SUB-QUESTION SEPARATION**: If you see questions with (a), (b), (c) sub-questions, EACH sub-question should be extracted as a SEPARATE question entry
- **INDIVIDUAL EXTRACTION**: Extract (a), (b), (c), (d), etc. as separate, independent questions
- **CONTEXT INCLUSION**: Include necessary context from the main question in each sub-question title
- **COMPLETE SUB-QUESTION**: Each sub-question should be self-contained with full context
- **NO DUPLICATES**: If you encounter the same question multiple times, extract it ONLY ONCE
- **UNIQUE IDENTIFICATION**: Each question should have unique content - do not repeat identical questions

### ğŸ¯ KNOWLEDGE POINT ANALYSIS RULES:
- Carefully analyze question content and find matching knowledge points from curriculum content arrays
- knowledge_points must be array format: ["knowledge point 1", "knowledge point 2", "knowledge point 3"]
- Prioritize selecting 1-3 most relevant specific knowledge points
- If no matching knowledge points found, use ["Uncategorized"]

### ğŸ¯ QUESTION IDENTIFICATION PATTERNS:
- Look for numbering: 1., 2., (1), (2), Q1, Question 1, etc.
- Look for sub-question markers: (a), (b), (c), (i), (ii), (iii), etc.
- Identify question marks, choice options, fill-in blanks
- Look for "Answer", "Solution", etc. keywords but DO NOT include answers in output
- **CRITICAL**: Extract EACH sub-question (a), (b), (c) as a SEPARATE question entry

### ğŸ¯ SUB-QUESTION EXTRACTION EXAMPLES:
- **Input**: "Three processes p1, p2 and p3... (a) What is... (b) Calculate... (c) Explain..."
- **Output**: THREE separate questions:
  - Question 1: "Three processes p1, p2 and p3... (a) What is..."
  - Question 2: "Three processes p1, p2 and p3... (b) Calculate..."
  - Question 3: "Three processes p1, p2 and p3... (c) Explain..."

### âš¡ OUTPUT REQUIREMENTS:
- Output JSON directly, do not wrap in ```json``` code blocks
- Do not add any explanatory text
- Ensure JSON format is completely correct and parseable
- If no questions found, output: {{"questions": []}}

Please carefully analyze this PDF document and extract all exam questions WITH proper sub-question separation:"""
        return prompt
    
    async def analyze_pdf_with_ai(self, pdf_path: str, pdf_name: str, max_retries: int = 3) -> Dict[str, Any]:
        """ä½¿ç”¨Google AIç›´æ¥åˆ†æPDFæ–‡ä»¶å¹¶æå–é¢˜ç›®"""
        for attempt in range(max_retries):
            try:
                # æ§åˆ¶APIè°ƒç”¨é€Ÿç‡
                self.wait_for_rate_limit()
                
                # åˆ›å»ºæç¤ºè¯
                prompt = self.create_extraction_prompt()
                
                self.logger.info(f"æ­£åœ¨åˆ†æ {pdf_name}... (å°è¯• {attempt + 1}/{max_retries})")
                
                # è¯»å–PDFæ–‡ä»¶ä¸ºå­—èŠ‚æ•°æ®
                pdf_path_obj = Path(pdf_path)
                pdf_bytes = pdf_path_obj.read_bytes()
                
                # ä½¿ç”¨æ–°çš„Google AI APIç›´æ¥å¤„ç†PDF
                response = await asyncio.to_thread(
                    self.client.models.generate_content,
                    model="gemini-2.5-pro",  
                    contents=[
                        types.Part.from_bytes(
                            data=pdf_bytes,
                            mime_type='application/pdf',
                        ),
                        prompt
                    ]
                )
                
                # ğŸ“‹ è¶…çº§æ™ºèƒ½JSONè§£æ - ä¸ºäº†ç™¾ä¸‡å¹´è–ªï¼
                response_text = response.text.strip()
                self.logger.info(f"AIå“åº”é•¿åº¦: {len(response_text)} å­—ç¬¦")
                
                # ğŸ” å¤šç§æ–¹å¼å°è¯•æå–JSON
                json_candidates = []
                
                # æ–¹å¼1: å¯»æ‰¾```jsonä»£ç å—
                if '```json' in response_text:
                    json_start = response_text.find('```json') + 7
                    json_end = response_text.find('```', json_start)
                    if json_end > json_start:
                        json_candidates.append(response_text[json_start:json_end].strip())
                
                # æ–¹å¼2: å¯»æ‰¾ç¬¬ä¸€ä¸ª{åˆ°æœ€åä¸€ä¸ª}
                if '{' in response_text and '}' in response_text:
                    json_start = response_text.find('{')
                    json_end = response_text.rfind('}') + 1
                    json_candidates.append(response_text[json_start:json_end])
                
                # æ–¹å¼3: å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•æ•´ä¸ªå“åº”
                json_candidates.append(response_text)
                
                # ğŸ¯ å°è¯•è§£ææ¯ä¸ªå€™é€‰JSON
                result = None
                for i, candidate in enumerate(json_candidates):
                    try:
                        result = json.loads(candidate)
                        self.logger.info(f"JSONè§£ææˆåŠŸ (æ–¹å¼ {i+1})")
                        break
                    except json.JSONDecodeError:
                        continue
                
                if result is None:
                    raise json.JSONDecodeError("æ‰€æœ‰JSONè§£ææ–¹å¼éƒ½å¤±è´¥", response_text, 0)
                
                # âœ… éªŒè¯å’Œä¿®å¤ç»“æœæ ¼å¼
                if not isinstance(result, dict):
                    result = {'questions': []}
                elif 'questions' not in result:
                    result = {'questions': []}
                elif not isinstance(result['questions'], list):
                    result = {'questions': []}
                
                # ğŸ”§ éªŒè¯æ¯ä¸ªé¢˜ç›®æ ¼å¼
                valid_questions = []
                for i, question in enumerate(result['questions']):
                    if isinstance(question, dict):
                        # ç¡®ä¿æ‰€æœ‰å¿…éœ€å­—æ®µå­˜åœ¨ï¼ˆç§»é™¤answerå­—æ®µï¼‰
                        fixed_question = {
                            'id': question.get('id', f'Q{i+1:03d}'),
                            'title': str(question.get('title', '')).strip(),
                            'type': str(question.get('type', 'Unknown')).strip(),
                            'refer': str(question.get('refer', 'Uncategorized')).strip(),
                            'knowledge_points': question.get('knowledge_points', ['Uncategorized'])
                        }
                        
                        # ç¡®ä¿knowledge_pointsæ˜¯åˆ—è¡¨æ ¼å¼
                        if not isinstance(fixed_question['knowledge_points'], list):
                            if isinstance(fixed_question['knowledge_points'], str):
                                fixed_question['knowledge_points'] = [fixed_question['knowledge_points']]
                            else:
                                fixed_question['knowledge_points'] = ['Uncategorized']
                        
                        # åªä¿ç•™æœ‰æ•ˆé¢˜ç›®ï¼ˆè‡³å°‘æœ‰æ ‡é¢˜ï¼‰
                        if fixed_question['title']:
                            valid_questions.append(fixed_question)
                
                result['questions'] = valid_questions
                
                self.logger.info(f"{pdf_name} è§£æå®Œæˆï¼Œæå–åˆ° {len(result['questions'])} é“é¢˜ç›®")
                return result
                
            except json.JSONDecodeError as e:
                self.logger.error(f"{pdf_name} JSONè§£æå¤±è´¥ (å°è¯• {attempt + 1}): {e}")
                if 'response_text' in locals():
                    self.logger.error(f"AIå“åº”å†…å®¹: {response_text[:500]}...")
                if attempt == max_retries - 1:
                    return {'questions': []}
            except Exception as e:
                error_msg = str(e)
                if "429" in error_msg or "quota" in error_msg.lower():
                    self.logger.warning(f"{pdf_name} é‡åˆ°é€Ÿç‡é™åˆ¶ (å°è¯• {attempt + 1}): {e}")
                    if attempt < max_retries - 1:
                        # ä»é”™è¯¯æ¶ˆæ¯ä¸­æå–é‡è¯•å»¶è¿Ÿ
                        import re
                        retry_match = re.search(r'retry in (\d+(?:\.\d+)?)s', error_msg)
                        if retry_match:
                            retry_delay = float(retry_match.group(1)) + 1  # æ·»åŠ é¢å¤–1ç§’ç¼“å†²
                            self.logger.info(f"ç­‰å¾… {retry_delay:.1f} ç§’åé‡è¯•...")
                            await asyncio.sleep(retry_delay)
                        else:
                            await asyncio.sleep(60)  # é»˜è®¤ç­‰å¾…60ç§’
                        continue
                else:
                    self.logger.error(f"{pdf_name} AIåˆ†æå¤±è´¥ (å°è¯• {attempt + 1}): {e}")
                    if attempt == max_retries - 1:
                        return {'questions': []}
        
        return {'questions': []}
    
    async def parse_single_pdf(self, pdf_path: str) -> Dict[str, Any]:
        """è§£æå•ä¸ªPDFæ–‡ä»¶"""
        pdf_name = Path(pdf_path).name
        self.logger.info(f"å¼€å§‹å¤„ç†: {pdf_name}")

        # æ£€æŸ¥PDFæ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”å¤§å°åˆé€‚
        pdf_file = Path(pdf_path)
        if not pdf_file.exists():
            self.logger.error(f"PDFæ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
            return {'questions': []}

        file_size_mb = pdf_file.stat().st_size / (1024 * 1024)
        if file_size_mb > 20:
            self.logger.warning(f"{pdf_name} æ–‡ä»¶è¿‡å¤§ ({file_size_mb:.1f}MB)ï¼Œå¯èƒ½ä¼šå¤„ç†å¤±è´¥")

        # ä½¿ç”¨AIç›´æ¥åˆ†æPDF
        result = await self.analyze_pdf_with_ai(pdf_path, pdf_name)

        # ä¸ºæ¯ä¸ªé¢˜ç›®æ·»åŠ æºæ–‡ä»¶ä¿¡æ¯
        for question in result.get('questions', []):
            question['source'] = pdf_name

        # ä¿å­˜å•ä¸ªPDFçš„è§£æç»“æœ
        self.save_single_pdf_result(result, pdf_name)

        return result
    
    def save_single_pdf_result(self, result: Dict[str, Any], pdf_name: str):
        """ä¿å­˜å•ä¸ªPDFçš„è§£æç»“æœ"""
        try:
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            output_dir = Path("output/pdf_results")
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶åï¼ˆç§»é™¤.pdfæ‰©å±•åï¼‰
            base_name = pdf_name.replace('.pdf', '')
            output_path = output_dir / f"{base_name}_result.json"
            
            # ä¿å­˜ç»“æœ
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"å•ä¸ªPDFç»“æœå·²ä¿å­˜: {output_path}")
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜å•ä¸ªPDFç»“æœå¤±è´¥ {pdf_name}: {e}")
    
    async def parse_all_pdfs(self, pdf_directory: str = ".", concurrency: int = 2) -> List[Dict[str, Any]]:
        """å¹¶å‘è§£ææ‰€æœ‰PDFæ–‡ä»¶ï¼ˆæ§åˆ¶å¹¶å‘æ•°é¿å…APIé€Ÿç‡é™åˆ¶ï¼‰"""
        pdf_files = list(Path(pdf_directory).glob("*.pdf"))
        
        if not pdf_files:
            self.logger.warning("æœªæ‰¾åˆ°PDFæ–‡ä»¶")
            return []
        
        self.logger.info(f"å‘ç° {len(pdf_files)} ä¸ªPDFæ–‡ä»¶")
        self.logger.info(f"ä½¿ç”¨å¹¶å‘å¤„ç†ï¼Œå¹¶å‘æ•°: {concurrency}")
        
        # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(concurrency)
        
        async def parse_with_semaphore(pdf_file: Path) -> Dict[str, Any]:
            """ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶çš„PDFè§£æå‡½æ•°"""
            async with semaphore:
                self.logger.info(f"å¼€å§‹å¤„ç†: {pdf_file.name}")
                result = await self.parse_single_pdf(str(pdf_file))
                self.logger.info(f"å®Œæˆå¤„ç†: {pdf_file.name} ({len(result.get('questions', []))} é“é¢˜ç›®)")
                return result
        
        # åˆ›å»ºæ‰€æœ‰è§£æä»»åŠ¡
        tasks = [parse_with_semaphore(pdf_file) for pdf_file in pdf_files]
        
        # ä½¿ç”¨è¿›åº¦æ¡æ˜¾ç¤ºå¤„ç†è¿›åº¦
        with tqdm(total=len(pdf_files), desc="è§£æPDFæ–‡ä»¶") as pbar:
            # åˆ†æ‰¹æ‰§è¡Œä»»åŠ¡ï¼Œé¿å…ä¸€æ¬¡æ€§åˆ›å»ºå¤ªå¤šä»»åŠ¡
            results = []
            batch_size = concurrency * 2  # æ‰¹æ¬¡å¤§å°ä¸ºå¹¶å‘æ•°çš„2å€
            
            for i in range(0, len(tasks), batch_size):
                batch_tasks = tasks[i:i + batch_size]
                batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
                
                # å¤„ç†ç»“æœ
                for result in batch_results:
                    if isinstance(result, Exception):
                        self.logger.error(f"PDFè§£æå‡ºç°å¼‚å¸¸: {result}")
                        results.append({'questions': []})  # æ·»åŠ ç©ºç»“æœ
                    else:
                        results.append(result)
                    
                    pbar.update(1)
                
                # æ‰¹æ¬¡é—´æ·»åŠ çŸ­æš‚å»¶è¿Ÿï¼Œé¿å…APIè¿‡è½½
                if i + batch_size < len(tasks):
                    await asyncio.sleep(1)
        
        return results
    
    def save_results(self, results: List[Dict[str, Any]], output_path: str = "output/parsed_questions.json"):
        """ä¿å­˜è§£æç»“æœåˆ°JSONæ–‡ä»¶"""
        # åˆå¹¶æ‰€æœ‰ç»“æœ
        all_questions = []
        for result in results:
            all_questions.extend(result.get('questions', []))
        
        # ä¸ºé¢˜ç›®é‡æ–°ç¼–å·
        for i, question in enumerate(all_questions, 1):
            question['id'] = f"Q{i:03d}"
        
        # ä¿å­˜ç»“æœ
        output_data = {
            'total_questions': len(all_questions),
            'questions': all_questions,
            'metadata': {
                'extracted_at': str(Path().resolve()),
                'pdf_count': len(results)
            }
        }
        
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"è§£æç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        self.logger.info(f"æ€»å…±æå–åˆ° {len(all_questions)} é“é¢˜ç›®")
        
        return output_data

async def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºPDFè§£æåŠŸèƒ½"""
    parser = PDFParser()
    
    # è§£ææ‰€æœ‰PDFæ–‡ä»¶
    results = await parser.parse_all_pdfs()
    
    # ä¿å­˜ç»“æœ
    parser.save_results(results)

if __name__ == "__main__":
    asyncio.run(main())